{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the environment and agent paramters\n",
    "num_of_col = 5\n",
    "num_of_row = 4\n",
    "num_of_element = num_of_col * num_of_row\n",
    "regular_reward_value = 0\n",
    "goal_reward_value = 100\n",
    "goal_col_index = 5\n",
    "goal_row_index = 4\n",
    "start_col_index = 1\n",
    "start_row_index = 1\n",
    "\n",
    "gamma = 0.9 # discount rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate the reward matrix in the grid\n",
    "# Grid position (i, j) is (col, row), 1-based index, start from (1, 1)\n",
    "# The state index is computed by the formula: \n",
    "# index = (j - 1) * num_of_columns + (i - 1) with (i, j) represent ith column and jth row in the grid\n",
    "# Example:\n",
    "# Grid environment with four rows and five column\n",
    "# Goal state is at (5, 4) whose state index is (4-1)*5 + (5 - 1) = 19\n",
    "# Start state is at (1,1) whose state index is (1-1)*5 + (1 - 1) = 0\n",
    "\n",
    "class GridStateMatrix:\n",
    "    def __init__(self, num_of_col, num_of_row, default_value=0):\n",
    "        self.num_of_col = num_of_col # number of columns of the grid environment (position)\n",
    "        self.num_of_row = num_of_row # number of rows of the grid environment (position)\n",
    "        self.num_of_state = self.num_of_col * self.num_of_row # number of states in the state-action matrix\n",
    "        self.default_value = default_value # default value for initial matrix\n",
    "        matrixValues = [self.default_value for _ in range(self.num_of_state * self.num_of_state)]\n",
    "        self.gridStateMatrix = np.matrix(matrixValues).reshape((self.num_of_state, self.num_of_state))\n",
    "        \n",
    "    def get_state_index(self, col_index, row_index):\n",
    "        return (row_index - 1) * num_of_col + (col_index - 1)\n",
    "    \n",
    "    def get_grid_position(self, state_index):\n",
    "        if (state_index >= self.num_of_state):\n",
    "            return -1, -1\n",
    "        row_index = state_index // self.num_of_col + 1\n",
    "        col_index = state_index % self.num_of_col + 1\n",
    "        return col_index, row_index\n",
    "    \n",
    "    def get_neighbors(self, col_index, row_index):\n",
    "        neighbors = list()\n",
    "        if (row_index < self.num_of_row): # up\n",
    "            neighbors.append((col_index, row_index+1))\n",
    "        if (row_index > 1): # down\n",
    "            neighbors.append((col_index, row_index-1))\n",
    "        if (col_index > 1): # left\n",
    "            neighbors.append((col_index-1, row_index))\n",
    "        if (col_index < self.num_of_col): # right\n",
    "            neighbors.append((col_index+1, row_index))\n",
    "        return neighbors\n",
    "    \n",
    "    def set_element_value(self, value, state_index, action_index):\n",
    "        if (state_index >= self.num_of_state):\n",
    "            return False\n",
    "        if (action_index >= self.num_of_state):\n",
    "            return False\n",
    "        self.gridStateMatrix[state_index, action_index] = value\n",
    "        return True\n",
    "\n",
    "    def get_state_matrix(self):\n",
    "        return self.gridStateMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Reward = GridStateMatrix(num_of_col,num_of_row, -1)\n",
    "\n",
    "# set up state-action relation between neighbor grid cell\n",
    "for col in range(num_of_col):\n",
    "    for row in range(num_of_row):\n",
    "        neighbors = Reward.get_neighbors(col+1, row+1)\n",
    "        s_index = Reward.get_state_index(col+1, row+1)\n",
    "        for i, j in neighbors:\n",
    "            #print col+1, row+1, '->', i, j\n",
    "            a_index = Reward.get_state_index(i, j)\n",
    "            Reward.set_element_value(regular_reward_value, s_index, a_index)\n",
    "\n",
    "relations = Reward.get_neighbors(goal_col_index, goal_row_index)\n",
    "a_index = Reward.get_state_index(goal_col_index, goal_row_index)\n",
    "for i, j in relations:\n",
    "    #print i, j\n",
    "    s_index = Reward.get_state_index(i,j)\n",
    "    Reward.set_element_value(goal_reward_value, s_index, a_index)\n",
    "Reward.set_element_value(goal_reward_value, a_index, a_index)\n",
    "        \n",
    "#print Reward.get_state_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize Q matrix\n",
    "Q_current = GridStateMatrix(num_of_col, num_of_row)\n",
    "#Q_prev = GridStateMatrix(num_of_col, num_of_row, 100)\n",
    "#print Q_current.get_state_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize learning parameters\n",
    "alpha = 1.0 # learning rate\n",
    "epsilon = 0.0 # explore probability threshold\n",
    "total_episode = 10 # total number of training episodes\n",
    "#deltaQ = Q_current.get_state_matrix() - Q_prev.get_state_matrix()\n",
    "#diffQ = np.asarray(deltaQ)*np.asarray(deltaQ)\n",
    "#print diffQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Repeat learning over episodes\n",
    "def get_max_Q_action(Q, s_index, epsilon=0.0):\n",
    "    s_col, s_row = Q.get_grid_position(s_index)\n",
    "    neighbors = Q.get_neighbors(s_col, s_row)\n",
    "    num_of_neighbors = len(neighbors)\n",
    "    a_max_index = -1\n",
    "    Q_max = -np.inf\n",
    "    for n_col, n_row in neighbors:\n",
    "        n_index = Q.get_state_index(n_col, n_row)\n",
    "        #print s_col, s_row, '->', n_col, n_row\n",
    "        #print s_index, '->', n_index\n",
    "        Q_n = Q.get_state_matrix()[s_index, n_index]\n",
    "        if (Q_n > Q_max):\n",
    "            Q_max = Q_n\n",
    "            a_max_index = n_index\n",
    "    if (rnd.random() < epsilon and num_of_neighbors!=0):\n",
    "        choice = rnd.randint(0, num_of_neighbors-1)\n",
    "        a_index = Q.get_state_index(neighbors[choice][0], neighbors[choice][1])\n",
    "    else:\n",
    "        a_index = a_max_index\n",
    "    return Q_max, a_max_index, a_index\n",
    "\n",
    "episode = 0\n",
    "goal_index = Q_current.get_state_index(goal_col_index, goal_row_index)\n",
    "while (episode < total_episode):\n",
    "    # Initialize the state at start state\n",
    "    episode += 1\n",
    "    s_col = start_col_index\n",
    "    s_row = start_row_index\n",
    "    s_index = Q_current.get_state_index(s_col, s_row)\n",
    "    # Check if current state is goal state\n",
    "    while (s_index != goal_index):\n",
    "        # Choose action from s using policy derived from Q (epsilon-greedy)\n",
    "        _, _, a_index = get_max_Q_action(Q_current, s_index, epsilon)\n",
    "        # Take action a, observe r and s'\n",
    "        next_reward = Reward.get_state_matrix()[s_index, a_index]\n",
    "        # update Q(s,a) <- Q(s,a) + alpha * [r + gamma*max_a' Q(s', a') - Q(s,a)]\n",
    "        next_Q, _, _ = get_max_Q_action(Q_current, a_index)\n",
    "        cur_Q = Q_current.get_state_matrix()[s_index, a_index] \n",
    "        new_Q = cur_Q + alpha*(next_reward + gamma*next_Q - cur_Q)\n",
    "        Q_current.set_element_value(new_Q, s_index, a_index)\n",
    "        # move to next state s <- s' \n",
    "        s_index = a_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q_1_1,_,_ = get_max_Q_action(Q_current, Q_current.get_state_index(1,1), 0.0)\n",
    "print Q_1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q_3_2,_,_ = get_max_Q_action(Q_current, Q_current.get_state_index(3,2), 0.0)\n",
    "print Q_3_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
