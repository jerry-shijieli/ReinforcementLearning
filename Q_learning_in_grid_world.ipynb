{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the environment and agent paramters\n",
    "num_of_col = 5\n",
    "num_of_row = 4\n",
    "num_of_element = num_of_col * num_of_row\n",
    "regular_reward_value = 0\n",
    "goal_reward_value = 100\n",
    "goal_col_index = 5\n",
    "goal_row_index = 4\n",
    "start_col_index = 1\n",
    "start_row_index = 1\n",
    "\n",
    "gamma = 0.9 # discount rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate the reward matrix in the grid\n",
    "# Grid position (i, j) is (col, row), 1-based index, start from (1, 1)\n",
    "# The state index is computed by the formula: \n",
    "# index = (j - 1) * num_of_columns + (i - 1) with (i, j) represent ith column and jth row in the grid\n",
    "# Example:\n",
    "# Grid environment with four rows and five column\n",
    "# Goal state is at (5, 4) whose state index is (4-1)*5 + (5 - 1) = 19\n",
    "# Start state is at (1,1) whose state index is (1-1)*5 + (1 - 1) = 0\n",
    "\n",
    "class GridStateMatrix:\n",
    "    def __init__(self, num_of_col, num_of_row, default_value=0):\n",
    "        self.num_of_col = num_of_col # number of columns of the grid environment (position)\n",
    "        self.num_of_row = num_of_row # number of rows of the grid environment (position)\n",
    "        self.num_of_state = self.num_of_col * self.num_of_row # number of states in the state-action matrix\n",
    "        self.default_value = default_value # default value for initial matrix\n",
    "        matrixValues = [self.default_value for _ in range(self.num_of_state * self.num_of_state)]\n",
    "        self.gridStateMatrix = np.matrix(matrixValues).reshape((self.num_of_state, self.num_of_state))\n",
    "        \n",
    "    def get_state_index(self, col_index, row_index):\n",
    "        return (row_index - 1) * num_of_col + (col_index - 1)\n",
    "    \n",
    "    def get_grid_position(self, state_index):\n",
    "        if (state_index >= self.num_of_state):\n",
    "            return -1, -1\n",
    "        row_index = state_index // self.num_of_col + 1\n",
    "        col_index = state_index % self.num_of_col + 1\n",
    "        return row_index, col_index\n",
    "    \n",
    "    def get_neighbors(self, col_index, row_index):\n",
    "        neighbors = list()\n",
    "        if (row_index < self.num_of_row): # up\n",
    "            neighbors.append((col_index, row_index+1))\n",
    "        if (row_index > 1): # down\n",
    "            neighbors.append((col_index, row_index-1))\n",
    "        if (col_index > 1): # left\n",
    "            neighbors.append((col_index-1, row_index))\n",
    "        if (col_index < self.num_of_col): # right\n",
    "            neighbors.append((col_index+1, row_index))\n",
    "        return neighbors\n",
    "    \n",
    "    def set_element_value(self, value, state_index, action_index):\n",
    "        if (state_index >= self.num_of_state):\n",
    "            return False\n",
    "        if (action_index >= self.num_of_state):\n",
    "            return False\n",
    "        self.gridStateMatrix[state_index, action_index] = value\n",
    "        return True\n",
    "\n",
    "    def get_state_matrix(self):\n",
    "        return self.gridStateMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 -> 1 2\n",
      "1 1 -> 2 1\n",
      "1 2 -> 1 3\n",
      "1 2 -> 1 1\n",
      "1 2 -> 2 2\n",
      "1 3 -> 1 4\n",
      "1 3 -> 1 2\n",
      "1 3 -> 2 3\n",
      "1 4 -> 1 3\n",
      "1 4 -> 2 4\n",
      "2 1 -> 2 2\n",
      "2 1 -> 1 1\n",
      "2 1 -> 3 1\n",
      "2 2 -> 2 3\n",
      "2 2 -> 2 1\n",
      "2 2 -> 1 2\n",
      "2 2 -> 3 2\n",
      "2 3 -> 2 4\n",
      "2 3 -> 2 2\n",
      "2 3 -> 1 3\n",
      "2 3 -> 3 3\n",
      "2 4 -> 2 3\n",
      "2 4 -> 1 4\n",
      "2 4 -> 3 4\n",
      "3 1 -> 3 2\n",
      "3 1 -> 2 1\n",
      "3 1 -> 4 1\n",
      "3 2 -> 3 3\n",
      "3 2 -> 3 1\n",
      "3 2 -> 2 2\n",
      "3 2 -> 4 2\n",
      "3 3 -> 3 4\n",
      "3 3 -> 3 2\n",
      "3 3 -> 2 3\n",
      "3 3 -> 4 3\n",
      "3 4 -> 3 3\n",
      "3 4 -> 2 4\n",
      "3 4 -> 4 4\n",
      "4 1 -> 4 2\n",
      "4 1 -> 3 1\n",
      "4 1 -> 5 1\n",
      "4 2 -> 4 3\n",
      "4 2 -> 4 1\n",
      "4 2 -> 3 2\n",
      "4 2 -> 5 2\n",
      "4 3 -> 4 4\n",
      "4 3 -> 4 2\n",
      "4 3 -> 3 3\n",
      "4 3 -> 5 3\n",
      "4 4 -> 4 3\n",
      "4 4 -> 3 4\n",
      "4 4 -> 5 4\n",
      "5 1 -> 5 2\n",
      "5 1 -> 4 1\n",
      "5 2 -> 5 3\n",
      "5 2 -> 5 1\n",
      "5 2 -> 4 2\n",
      "5 3 -> 5 4\n",
      "5 3 -> 5 2\n",
      "5 3 -> 4 3\n",
      "5 4 -> 5 3\n",
      "5 4 -> 4 4\n",
      "5 3\n",
      "4 4\n",
      "[[ -1   0  -1  -1  -1   0  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1]\n",
      " [  0  -1   0  -1  -1  -1   0  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1]\n",
      " [ -1   0  -1   0  -1  -1  -1   0  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1]\n",
      " [ -1  -1   0  -1   0  -1  -1  -1   0  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1]\n",
      " [ -1  -1  -1   0  -1  -1  -1  -1  -1   0  -1  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1]\n",
      " [  0  -1  -1  -1  -1  -1   0  -1  -1  -1   0  -1  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1]\n",
      " [ -1   0  -1  -1  -1   0  -1   0  -1  -1  -1   0  -1  -1  -1  -1  -1  -1\n",
      "   -1  -1]\n",
      " [ -1  -1   0  -1  -1  -1   0  -1   0  -1  -1  -1   0  -1  -1  -1  -1  -1\n",
      "   -1  -1]\n",
      " [ -1  -1  -1   0  -1  -1  -1   0  -1   0  -1  -1  -1   0  -1  -1  -1  -1\n",
      "   -1  -1]\n",
      " [ -1  -1  -1  -1   0  -1  -1  -1   0  -1  -1  -1  -1  -1   0  -1  -1  -1\n",
      "   -1  -1]\n",
      " [ -1  -1  -1  -1  -1   0  -1  -1  -1  -1  -1   0  -1  -1  -1   0  -1  -1\n",
      "   -1  -1]\n",
      " [ -1  -1  -1  -1  -1  -1   0  -1  -1  -1   0  -1   0  -1  -1  -1   0  -1\n",
      "   -1  -1]\n",
      " [ -1  -1  -1  -1  -1  -1  -1   0  -1  -1  -1   0  -1   0  -1  -1  -1   0\n",
      "   -1  -1]\n",
      " [ -1  -1  -1  -1  -1  -1  -1  -1   0  -1  -1  -1   0  -1   0  -1  -1  -1\n",
      "    0  -1]\n",
      " [ -1  -1  -1  -1  -1  -1  -1  -1  -1   0  -1  -1  -1   0  -1  -1  -1  -1\n",
      "   -1 100]\n",
      " [ -1  -1  -1  -1  -1  -1  -1  -1  -1  -1   0  -1  -1  -1  -1  -1   0  -1\n",
      "   -1  -1]\n",
      " [ -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1   0  -1  -1  -1   0  -1   0\n",
      "   -1  -1]\n",
      " [ -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1   0  -1  -1  -1   0  -1\n",
      "    0  -1]\n",
      " [ -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1   0  -1  -1  -1   0\n",
      "   -1 100]\n",
      " [ -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1   0  -1  -1  -1\n",
      "    0 100]]\n"
     ]
    }
   ],
   "source": [
    "Reward = GridStateMatrix(num_of_col,num_of_row, -1)\n",
    "\n",
    "# set up state-action relation between neighbor grid cell\n",
    "for col in range(num_of_col):\n",
    "    for row in range(num_of_row):\n",
    "        neighbors = Reward.get_neighbors(col+1, row+1)\n",
    "        s_index = Reward.get_state_index(col+1, row+1)\n",
    "        for i, j in neighbors:\n",
    "            print col+1, row+1, '->', i, j\n",
    "            a_index = Reward.get_state_index(i, j)\n",
    "            Reward.set_element_value(regular_reward_value, s_index, a_index)\n",
    "\n",
    "relations = Reward.get_neighbors(goal_col_index, goal_row_index)\n",
    "a_index = Reward.get_state_index(goal_col_index, goal_row_index)\n",
    "for i, j in relations:\n",
    "    print i, j\n",
    "    s_index = Reward.get_state_index(i,j)\n",
    "    Reward.set_element_value(goal_reward_value, s_index, a_index)\n",
    "Reward.set_element_value(goal_reward_value, a_index, a_index)\n",
    "        \n",
    "print Reward.get_state_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# initialize Q matrix\n",
    "Q_current = GridStateMatrix(num_of_col, num_of_row)\n",
    "#Q_prev = GridStateMatrix(num_of_col, num_of_row, 100)\n",
    "print Q_current.get_state_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize learning parameters\n",
    "alpha = 1.0 # learning rate\n",
    "epsilon = 0.1 # explore probability threshold\n",
    "total_episode = 100 # total number of training episodes\n",
    "#deltaQ = Q_current.get_state_matrix() - Q_prev.get_state_matrix()\n",
    "#diffQ = np.asarray(deltaQ)*np.asarray(deltaQ)\n",
    "#print diffQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Repeat learning over episodes\n",
    "episode = 0\n",
    "while (episode < total_episode):\n",
    "    # Initialize the state at start state\n",
    "    \n",
    "    # Check if current state is goal state\n",
    "    \n",
    "        # Choose action from s using policy derived from Q (epsilon-greedy)\n",
    "        \n",
    "        # Take action a, observe r and s'\n",
    "        \n",
    "        # update Q(s,a) <- Q(s,a) + alpha * [r + gamma*max_a' Q(s', a') - Q(s,a)]\n",
    "        \n",
    "        # move to next state s <- s' "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
