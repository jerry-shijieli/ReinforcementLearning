{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Shijie Li(Jerry)\n",
    "Reference: [Q-Learning Tutorial](http://mnemstudio.org/path-finding-q-learning-tutorial.htm) and [Temporal Differ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rnd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the environment and agent paramters\n",
    "num_of_col = 5\n",
    "num_of_row = 4\n",
    "num_of_element = num_of_col * num_of_row\n",
    "regular_reward_value = 0\n",
    "goal_reward_value = 100\n",
    "goal_col_index = 5\n",
    "goal_row_index = 4\n",
    "start_col_index = 1\n",
    "start_row_index = 1\n",
    "\n",
    "gamma = 0.9 # discount rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate the reward matrix in the grid\n",
    "# Grid position (i, j) is (col, row), 1-based index, start from (1, 1)\n",
    "# The state index is computed by the formula: \n",
    "# index = (j - 1) * num_of_columns + (i - 1) with (i, j) represent ith column and jth row in the grid\n",
    "# Example:\n",
    "# Grid environment with four rows and five column\n",
    "# Goal state is at (5, 4) whose state index is (4-1)*5 + (5 - 1) = 19\n",
    "# Start state is at (1,1) whose state index is (1-1)*5 + (1 - 1) = 0\n",
    "\n",
    "class GridStateMatrix:\n",
    "    def __init__(self, num_of_col, num_of_row, default_value=0):\n",
    "        self.num_of_col = num_of_col # number of columns of the grid environment (position)\n",
    "        self.num_of_row = num_of_row # number of rows of the grid environment (position)\n",
    "        self.num_of_state = self.num_of_col * self.num_of_row # number of states in the state-action matrix\n",
    "        self.default_value = default_value # default value for initial matrix\n",
    "        matrixValues = [self.default_value for _ in range(self.num_of_state * self.num_of_state)]\n",
    "        self.gridStateMatrix = np.matrix(matrixValues).reshape((self.num_of_state, self.num_of_state))\n",
    "        \n",
    "    def get_state_index(self, col_index, row_index):\n",
    "        return (row_index - 1) * num_of_col + (col_index - 1)\n",
    "    \n",
    "    def get_grid_position(self, state_index):\n",
    "        if (state_index >= self.num_of_state):\n",
    "            return -1, -1\n",
    "        row_index = state_index // self.num_of_col + 1\n",
    "        col_index = state_index % self.num_of_col + 1\n",
    "        return col_index, row_index\n",
    "    \n",
    "    def get_neighbors(self, col_index, row_index):\n",
    "        neighbors = list()\n",
    "        if (row_index < self.num_of_row): # up\n",
    "            neighbors.append((col_index, row_index+1))\n",
    "        if (row_index > 1): # down\n",
    "            neighbors.append((col_index, row_index-1))\n",
    "        if (col_index > 1): # left\n",
    "            neighbors.append((col_index-1, row_index))\n",
    "        if (col_index < self.num_of_col): # right\n",
    "            neighbors.append((col_index+1, row_index))\n",
    "        return neighbors\n",
    "    \n",
    "    def set_element_value(self, value, state_index, action_index):\n",
    "        if (state_index >= self.num_of_state):\n",
    "            return False\n",
    "        if (action_index >= self.num_of_state):\n",
    "            return False\n",
    "        self.gridStateMatrix[state_index, action_index] = value\n",
    "        return True\n",
    "\n",
    "    def get_state_matrix(self):\n",
    "        return self.gridStateMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Reward = GridStateMatrix(num_of_col,num_of_row, np.inf)\n",
    "\n",
    "# set up state-action relation between neighbor grid cell\n",
    "for col in range(num_of_col):\n",
    "    for row in range(num_of_row):\n",
    "        neighbors = Reward.get_neighbors(col+1, row+1)\n",
    "        s_index = Reward.get_state_index(col+1, row+1)\n",
    "        for i, j in neighbors:\n",
    "            #print col+1, row+1, '->', i, j\n",
    "            a_index = Reward.get_state_index(i, j)\n",
    "            Reward.set_element_value(regular_reward_value, s_index, a_index)\n",
    "\n",
    "relations = Reward.get_neighbors(goal_col_index, goal_row_index)\n",
    "a_index = Reward.get_state_index(goal_col_index, goal_row_index)\n",
    "for i, j in relations:\n",
    "    #print i, j\n",
    "    s_index = Reward.get_state_index(i,j)\n",
    "    Reward.set_element_value(goal_reward_value, s_index, a_index)\n",
    "if not Reward.set_element_value(goal_reward_value, a_index, a_index):\n",
    "    print 'Goal reward value setting failed!'\n",
    "        \n",
    "#print Reward.get_state_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find the max Q value over all possible actions at the given state\n",
    "def get_max_Q_action(Q, s_index, epsilon=0.0):\n",
    "    s_col, s_row = Q.get_grid_position(s_index)\n",
    "    neighbors = Q.get_neighbors(s_col, s_row)\n",
    "    rnd.shuffle(neighbors)\n",
    "    num_of_neighbors = len(neighbors)\n",
    "    a_max_index = -1\n",
    "    Q_max = -np.inf\n",
    "    for n_col, n_row in neighbors:\n",
    "        n_index = Q.get_state_index(n_col, n_row)\n",
    "        #print s_col, s_row, '->', n_col, n_row\n",
    "        #print s_index, '->', n_index\n",
    "        Q_n = Q.get_state_matrix()[s_index, n_index]\n",
    "        if (Q_n > Q_max):\n",
    "            Q_max = Q_n\n",
    "            a_max_index = n_index\n",
    "    if (rnd.random() < epsilon and num_of_neighbors!=0):\n",
    "        choice = rnd.randint(0, num_of_neighbors-1)\n",
    "        a_index = Q.get_state_index(neighbors[choice][0], neighbors[choice][1])\n",
    "    else:\n",
    "        a_index = a_max_index\n",
    "    return Q_max, a_max_index, a_index\n",
    "\n",
    "\n",
    "def q_learning_process(Reward, start_state_index, goal_state_index, goal_reward_value, alpha, epsilon=0.0, total_episode=100):\n",
    "    num_of_state = Reward.get_state_matrix().shape[0]\n",
    "    num_of_col, num_of_row = Reward.get_grid_position(num_of_state-1)\n",
    "    # initialize Q matrix and V matrix\n",
    "    Q_current = GridStateMatrix(num_of_col, num_of_row)\n",
    "    V_values = [0 for _ in range(num_of_col*num_of_row)]\n",
    "    V_matrix = np.matrix(V_values).reshape((num_of_row, num_of_col))\n",
    "    goal_col, goal_row = Q_current.get_grid_position(goal_state_index)\n",
    "    V_matrix[goal_row-1, goal_col-1] = goal_reward_value\n",
    "    reward_values = [0 for _ in range(num_of_col*num_of_row)]\n",
    "    reward_matrix = np.matrix(reward_values).reshape((num_of_row, num_of_col))\n",
    "    reward_matrix[goal_row-1, goal_col-1] = goal_reward_value\n",
    "    # start learning over episodes\n",
    "    episode = 0\n",
    "    while (episode < total_episode):\n",
    "        # Initialize the state at start state\n",
    "        #print episode\n",
    "        episode += 1\n",
    "        s_index = start_state_index\n",
    "        _, _, a_index = get_max_Q_action(Q_current, s_index, epsilon)\n",
    "        # Check if current state is goal state\n",
    "        while (s_index != goal_state_index):\n",
    "            # Choose action from s using policy derived from Q (epsilon-greedy)\n",
    "            _, _, a_index = get_max_Q_action(Q_current, s_index, epsilon)\n",
    "            # Take action a, observe r and s'\n",
    "            next_reward = Reward.get_state_matrix()[s_index, a_index]\n",
    "            # update Q(s,a) <- Q(s,a) + alpha * [r + gamma*max_a' Q(s', a') - Q(s,a)]\n",
    "            next_Q, _, _ = get_max_Q_action(Q_current, a_index)\n",
    "            cur_Q = Q_current.get_state_matrix()[s_index, a_index] \n",
    "            new_Q = cur_Q + alpha*(next_reward + gamma*next_Q - cur_Q)\n",
    "            Q_current.set_element_value(new_Q, s_index, a_index)\n",
    "            # update V(s) <- V(s) + alpha * [r + gamma*V(s') - V(s)]\n",
    "            next_col, next_row = Q_current.get_grid_position(a_index)\n",
    "            cur_col, cur_row = Q_current.get_grid_position(s_index)\n",
    "            next_V = V_matrix[next_row-1, next_col-1]\n",
    "            cur_V = V_matrix[cur_row-1, cur_col-1]\n",
    "            cur_reward = reward_matrix[cur_row-1, cur_col-1]\n",
    "            V_matrix[cur_row-1, cur_col-1] = cur_V + alpha*(cur_reward + gamma*next_V - cur_V)\n",
    "            # move to next state s <- s' \n",
    "            s_index = a_index\n",
    "    return Q_current, V_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize learning parameters\n",
    "alpha = 1.0 # learning rate\n",
    "epsilon = 0.0 # explore probability threshold\n",
    "total_episode = 10 # total number of training episodes\n",
    "start_state_index = Reward.get_state_index(start_col_index, start_row_index)\n",
    "goal_state_index = Reward.get_state_index(goal_col_index, goal_row_index)\n",
    "Q_current, V_matrix = q_learning_process(Reward, start_state_index, goal_state_index, goal_reward_value, alpha, epsilon, total_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "Q_1_1,_,_ = get_max_Q_action(Q_current, Q_current.get_state_index(1,1), 0.0)\n",
    "print Q_1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "Q_3_2,_,_ = get_max_Q_action(Q_current, Q_current.get_state_index(3,2), 0.0)\n",
    "print Q_3_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 45   0   0  64   0]\n",
      " [ 51  57  64  72   0]\n",
      " [  0   0   0  81   0]\n",
      " [  0  72  81  90 100]]\n"
     ]
    }
   ],
   "source": [
    "print V_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (1, 2), (2, 2), (3, 2), (4, 2), (4, 3), (4, 4), (5, 4)]\n"
     ]
    }
   ],
   "source": [
    "def retrieve_optimal_policy(Q, start_col, start_row, goal_col_index, goal_row_index):\n",
    "    action_trace = list()\n",
    "    s_index = Q.get_state_index(start_col, start_row)\n",
    "    g_index = Q.get_state_index(goal_col_index, goal_row_index)\n",
    "    action_trace.append((start_col, start_row))\n",
    "    while (s_index != g_index):\n",
    "        _,_,a_index = get_max_Q_action(Q, s_index)\n",
    "        action_trace.append(Q.get_grid_position(a_index))\n",
    "        s_index = a_index\n",
    "    return action_trace\n",
    "\n",
    "print retrieve_optimal_policy(Q_current, start_col_index, start_row_index, goal_col_index, goal_row_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Reward2 = copy.deepcopy(Reward)\n",
    "# insert another goal reward at the lower right corner (5,1)\n",
    "goal2_col_index = 5\n",
    "goal2_row_index = 1\n",
    "goal2_reward_value = 100\n",
    "relations = Reward2.get_neighbors(goal2_col_index, goal2_row_index)\n",
    "a_index = Reward2.get_state_index(goal2_col_index, goal2_row_index)\n",
    "for i, j in relations:\n",
    "    #print i, j\n",
    "    s_index = Reward2.get_state_index(i,j)\n",
    "    Reward2.set_element_value(goal2_reward_value, s_index, a_index)\n",
    "if not Reward2.set_element_value(goal2_reward_value, a_index, a_index):\n",
    "    print 'Goal reward value setting failed!'\n",
    "#print Reward2.get_state_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q_learning_multigoals(Reward, start_state_index, goal_state_index_list, goal_reward_list, alpha, epsilon=0.0, total_episode=100):\n",
    "    num_of_state = Reward.get_state_matrix().shape[0]\n",
    "    num_of_col, num_of_row = Reward.get_grid_position(num_of_state-1)\n",
    "    # initialize Q matrix\n",
    "    Q_current = GridStateMatrix(num_of_col, num_of_row)\n",
    "    V_values = [0 for _ in range(num_of_col*num_of_row)]\n",
    "    V_matrix = np.matrix(V_values).reshape((num_of_row, num_of_col))\n",
    "    reward_values = [0 for _ in range(num_of_col*num_of_row)]\n",
    "    reward_matrix = np.matrix(reward_values).reshape((num_of_row, num_of_col))\n",
    "    for g_index, g_reward in zip(goal_state_index_list, goal_reward_list):\n",
    "        g_col, g_row = Q_current.get_grid_position(g_index)\n",
    "        V_matrix[g_row-1, g_col-1] = g_reward\n",
    "        reward_matrix[g_row-1, g_col-1] = g_reward\n",
    "    episode = 0\n",
    "    while (episode < total_episode):\n",
    "        # Initialize the state at start state\n",
    "        #print episode\n",
    "        episode += 1\n",
    "        s_index = start_state_index\n",
    "        _, _, a_index = get_max_Q_action(Q_current, s_index, epsilon)\n",
    "        # Check if current state is goal state\n",
    "        while (s_index not in goal_state_index_list):\n",
    "            # Choose action from s using policy derived from Q (epsilon-greedy)\n",
    "            _, _, a_index = get_max_Q_action(Q_current, s_index, epsilon)\n",
    "            # Take action a, observe r and s'\n",
    "            next_reward = Reward.get_state_matrix()[s_index, a_index]\n",
    "            # update Q(s,a) <- Q(s,a) + alpha * [r + gamma*max_a' Q(s', a') - Q(s,a)]\n",
    "            next_Q, _, _ = get_max_Q_action(Q_current, a_index)\n",
    "            cur_Q = Q_current.get_state_matrix()[s_index, a_index] \n",
    "            new_Q = cur_Q + alpha*(next_reward + gamma*next_Q - cur_Q)\n",
    "            Q_current.set_element_value(new_Q, s_index, a_index)\n",
    "            # update V(s) <- V(s) + alpha * [r + gamma*V(s') - V(s)]\n",
    "            next_col, next_row = Q_current.get_grid_position(a_index)\n",
    "            cur_col, cur_row = Q_current.get_grid_position(s_index)\n",
    "            next_V = V_matrix[next_row-1, next_col-1]\n",
    "            cur_V = V_matrix[cur_row-1, cur_col-1]\n",
    "            cur_reward = reward_matrix[cur_row-1, cur_col-1]\n",
    "            V_matrix[cur_row-1, cur_col-1] = cur_V + alpha*(cur_reward + gamma*next_V - cur_V)\n",
    "            # move to next state s <- s' \n",
    "            s_index = a_index\n",
    "    return Q_current, V_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 64  72  81  90 100]\n",
      " [  0   0  72   0   0]\n",
      " [  0   0   0  81  90]\n",
      " [  0   0  81  90 100]]\n"
     ]
    }
   ],
   "source": [
    "# initialize learning parameters\n",
    "alpha = 1.0 # learning rate\n",
    "epsilon = 0.0 # explore probability threshold\n",
    "total_episode = 10 # total number of training episodes\n",
    "start_state_index = Reward2.get_state_index(start_col_index, start_row_index)\n",
    "goal_state_index = Reward2.get_state_index(goal_col_index, goal_row_index)\n",
    "goal2_state_index = Reward2.get_state_index(goal2_col_index, goal2_row_index)\n",
    "goal_state_index_list = [goal_state_index, goal2_state_index]\n",
    "goal_reward_list = [goal_reward_value, goal2_reward_value]\n",
    "Q_current2, V_matrix2 = q_learning_multigoals(Reward2, start_state_index, goal_state_index_list, goal_reward_list, alpha, epsilon, total_episode)\n",
    "print V_matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (5, 2), (4, 2), (3, 2), (3, 1), (4, 1), (5, 1), (5, 2), (5, 1), (4, 1), (5, 1), (4, 1), (5, 1), (4, 1), (5, 1), (5, 2), (5, 1), (4, 1), (5, 1), (4, 1), (5, 1), (4, 1), (5, 1), (4, 1), (5, 1), (5, 2), (4, 2), (4, 1), (5, 1), (5, 2), (5, 1), (5, 2), (5, 3), (5, 4)]\n"
     ]
    }
   ],
   "source": [
    "print retrieve_optimal_policy(Q_current2, start_col_index, start_row_index, goal_col_index, goal_row_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]\n"
     ]
    }
   ],
   "source": [
    "print retrieve_optimal_policy(Q_current2, start_col_index, start_row_index, goal2_col_index, goal2_row_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Reward3 = copy.deepcopy(Reward)\n",
    "# replace second goal reward by negative value at the lower right corner (5,1)\n",
    "goal3_col_index = 5\n",
    "goal3_row_index = 1\n",
    "goal3_reward_value = -100 # replaced value\n",
    "relations = Reward3.get_neighbors(goal3_col_index, goal3_row_index)\n",
    "a_index = Reward3.get_state_index(goal3_col_index, goal3_row_index)\n",
    "for i, j in relations:\n",
    "    #print i, j\n",
    "    s_index = Reward3.get_state_index(i,j)\n",
    "    Reward3.set_element_value(goal3_reward_value, s_index, a_index)\n",
    "if not Reward2.set_element_value(goal3_reward_value, a_index, a_index):\n",
    "    print 'Goal reward value setting failed!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  45    0    0    0 -100]\n",
      " [  51    0    0    0    0]\n",
      " [  57   64   72    0    0]\n",
      " [   0    0   81   90  100]]\n"
     ]
    }
   ],
   "source": [
    "# initialize learning parameters\n",
    "alpha = 1.0 # learning rate\n",
    "epsilon = 0.0 # explore probability threshold\n",
    "total_episode = 10 # total number of training episodes\n",
    "start_state_index = Reward3.get_state_index(start_col_index, start_row_index)\n",
    "goal_state_index = Reward3.get_state_index(goal_col_index, goal_row_index)\n",
    "goal3_state_index = Reward3.get_state_index(goal3_col_index, goal3_row_index)\n",
    "goal_state_index_list = [goal_state_index, goal3_state_index]\n",
    "goal_reward_list = [goal_reward_value, goal3_reward_value]\n",
    "Q_current3, V_matrix3 = q_learning_multigoals(Reward3, start_state_index, goal_state_index_list, goal_reward_list,alpha, epsilon, total_episode)\n",
    "print V_matrix3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (1, 2), (1, 3), (2, 3), (3, 3), (3, 4), (4, 4), (5, 4)]\n"
     ]
    }
   ],
   "source": [
    "print retrieve_optimal_policy(Q_current3, start_col_index, start_row_index, goal_col_index, goal_row_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
