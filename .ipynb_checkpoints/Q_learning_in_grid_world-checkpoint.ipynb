{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the environment and agent paramters\n",
    "num_of_col = 5\n",
    "num_of_row = 4\n",
    "num_of_element = num_of_col * num_of_row\n",
    "regular_reward_value = 0\n",
    "goal_reward_value = 100\n",
    "goal_col_index = 5\n",
    "goal_row_index = 4\n",
    "start_col_index = 1\n",
    "start_row_index = 1\n",
    "\n",
    "gamma = 0.9 # discount rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate the reward matrix in the grid\n",
    "# Grid position (i, j) is (col, row), 1-based index, start from (1, 1)\n",
    "# The state index is computed by the formula: \n",
    "# index = (j - 1) * num_of_columns + (i - 1) with (i, j) represent ith column and jth row in the grid\n",
    "# Example:\n",
    "# Grid environment with four rows and five column\n",
    "# Goal state is at (5, 4) whose state index is (4-1)*5 + (5 - 1) = 19\n",
    "# Start state is at (1,1) whose state index is (1-1)*5 + (1 - 1) = 0\n",
    "\n",
    "class GridStateMatrix:\n",
    "    def __init__(self, num_of_col, num_of_row, default_value=0):\n",
    "        self.num_of_col = num_of_col # number of columns of the grid environment (position)\n",
    "        self.num_of_row = num_of_row # number of rows of the grid environment (position)\n",
    "        self.num_of_state = self.num_of_col * self.num_of_row # number of states in the state-action matrix\n",
    "        self.default_value = default_value # default value for initial matrix\n",
    "        matrixValues = [self.default_value for _ in range(self.num_of_state * self.num_of_state)]\n",
    "        self.gridStateMatrix = np.matrix(matrixValues).reshape((self.num_of_state, self.num_of_state))\n",
    "        \n",
    "    def get_state_index(self, col_index, row_index):\n",
    "        return (row_index - 1) * num_of_col + (col_index - 1)\n",
    "    \n",
    "    def get_grid_position(self, state_index):\n",
    "        if (state_index >= self.num_of_state):\n",
    "            return -1, -1\n",
    "        row_index = state_index // self.num_of_col + 1\n",
    "        col_index = state_index % self.num_of_col + 1\n",
    "        return col_index, row_index\n",
    "    \n",
    "    def get_neighbors(self, col_index, row_index):\n",
    "        neighbors = list()\n",
    "        if (row_index < self.num_of_row): # up\n",
    "            neighbors.append((col_index, row_index+1))\n",
    "        if (row_index > 1): # down\n",
    "            neighbors.append((col_index, row_index-1))\n",
    "        if (col_index > 1): # left\n",
    "            neighbors.append((col_index-1, row_index))\n",
    "        if (col_index < self.num_of_col): # right\n",
    "            neighbors.append((col_index+1, row_index))\n",
    "        return neighbors\n",
    "    \n",
    "    def set_element_value(self, value, state_index, action_index):\n",
    "        if (state_index >= self.num_of_state):\n",
    "            return False\n",
    "        if (action_index >= self.num_of_state):\n",
    "            return False\n",
    "        self.gridStateMatrix[state_index, action_index] = value\n",
    "        return True\n",
    "\n",
    "    def get_state_matrix(self):\n",
    "        return self.gridStateMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Reward = GridStateMatrix(num_of_col,num_of_row, -1)\n",
    "\n",
    "# set up state-action relation between neighbor grid cell\n",
    "for col in range(num_of_col):\n",
    "    for row in range(num_of_row):\n",
    "        neighbors = Reward.get_neighbors(col+1, row+1)\n",
    "        s_index = Reward.get_state_index(col+1, row+1)\n",
    "        for i, j in neighbors:\n",
    "            #print col+1, row+1, '->', i, j\n",
    "            a_index = Reward.get_state_index(i, j)\n",
    "            Reward.set_element_value(regular_reward_value, s_index, a_index)\n",
    "\n",
    "relations = Reward.get_neighbors(goal_col_index, goal_row_index)\n",
    "a_index = Reward.get_state_index(goal_col_index, goal_row_index)\n",
    "for i, j in relations:\n",
    "    #print i, j\n",
    "    s_index = Reward.get_state_index(i,j)\n",
    "    Reward.set_element_value(goal_reward_value, s_index, a_index)\n",
    "if not Reward.set_element_value(goal_reward_value, a_index, a_index):\n",
    "    print 'Goal reward value setting failed!'\n",
    "        \n",
    "#print Reward.get_state_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Repeat learning over episodes\n",
    "def get_max_Q_action(Q, s_index, epsilon=0.0):\n",
    "    s_col, s_row = Q.get_grid_position(s_index)\n",
    "    neighbors = Q.get_neighbors(s_col, s_row)\n",
    "    rnd.shuffle(neighbors)\n",
    "    num_of_neighbors = len(neighbors)\n",
    "    a_max_index = -1\n",
    "    Q_max = -np.inf\n",
    "    for n_col, n_row in neighbors:\n",
    "        n_index = Q.get_state_index(n_col, n_row)\n",
    "        #print s_col, s_row, '->', n_col, n_row\n",
    "        #print s_index, '->', n_index\n",
    "        Q_n = Q.get_state_matrix()[s_index, n_index]\n",
    "        if (Q_n > Q_max):\n",
    "            Q_max = Q_n\n",
    "            a_max_index = n_index\n",
    "    if (rnd.random() < epsilon and num_of_neighbors!=0):\n",
    "        choice = rnd.randint(0, num_of_neighbors-1)\n",
    "        a_index = Q.get_state_index(neighbors[choice][0], neighbors[choice][1])\n",
    "    else:\n",
    "        a_index = a_max_index\n",
    "    return Q_max, a_max_index, a_index\n",
    "\n",
    "\n",
    "def q_learning_process(Reward, start_state_index, goal_state_index, alpha, epsilon=0.0, total_episode=100):\n",
    "    num_of_state = Reward.get_state_matrix().shape[0]\n",
    "    num_of_col, num_of_row = Reward.get_grid_position(num_of_state)\n",
    "    # initialize Q matrix\n",
    "    Q_current = GridStateMatrix(num_of_col, num_of_row)\n",
    "    episode = 0\n",
    "    while (episode < total_episode):\n",
    "        # Initialize the state at start state\n",
    "        #print episode\n",
    "        episode += 1\n",
    "        s_index = start_state_index\n",
    "        a_index = -1\n",
    "        # Check if current state is goal state\n",
    "        while (s_index != goal_state_index):\n",
    "            print s_index, '->', a_index\n",
    "            # Choose action from s using policy derived from Q (epsilon-greedy)\n",
    "            _, _, a_index = get_max_Q_action(Q_current, s_index, epsilon)\n",
    "            # Take action a, observe r and s'\n",
    "            next_reward = Reward.get_state_matrix()[s_index, a_index]\n",
    "            # update Q(s,a) <- Q(s,a) + alpha * [r + gamma*max_a' Q(s', a') - Q(s,a)]\n",
    "            next_Q, _, _ = get_max_Q_action(Q_current, a_index)\n",
    "            cur_Q = Q_current.get_state_matrix()[s_index, a_index] \n",
    "            new_Q = cur_Q + alpha*(next_reward + gamma*next_Q - cur_Q)\n",
    "            Q_current.set_element_value(new_Q, s_index, a_index)\n",
    "            # move to next state s <- s' \n",
    "            s_index = a_index\n",
    "    return Q_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> -1\n",
      "-1 -> -1\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "cannot convert float infinity to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ee7cc5689a63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstart_state_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_col_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_row_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgoal_state_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgoal_col_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal_row_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mQ_current\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_learning_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mReward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal_state_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_episode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-dff25263a416>\u001b[0m in \u001b[0;36mq_learning_process\u001b[0;34m(Reward, start_state_index, goal_state_index, alpha, epsilon, total_episode)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mcur_Q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ_current\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mnew_Q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_Q\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_reward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnext_Q\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcur_Q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mQ_current\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_element_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_Q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0;31m# move to next state s <- s'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0ms_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-f9e52192ce76>\u001b[0m in \u001b[0;36mset_element_value\u001b[0;34m(self, value, state_index, action_index)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maction_index\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_of_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgridStateMatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOverflowError\u001b[0m: cannot convert float infinity to integer"
     ]
    }
   ],
   "source": [
    "# initialize learning parameters\n",
    "alpha = 1.0 # learning rate\n",
    "epsilon = 0.0 # explore probability threshold\n",
    "total_episode = 10000 # total number of training episodes\n",
    "start_state_index = Reward.get_state_index(start_col_index, start_row_index)\n",
    "goal_state_index = Reward.get_state_index(goal_col_index, goal_row_index)\n",
    "Q_current = q_learning_process(Reward, start_state_index, goal_state_index, alpha, total_episode=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q_1_1,_,_ = get_max_Q_action(Q_current, Q_current.get_state_index(1,1), 0.0)\n",
    "print Q_1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q_3_2,_,_ = get_max_Q_action(Q_current, Q_current.get_state_index(3,2), 0.0)\n",
    "print Q_3_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def retrieve_optimal_policy(Q, start_col, start_row, goal_col_index, goal_row_index):\n",
    "    action_trace = list()\n",
    "    s_index = Q.get_state_index(start_col, start_row)\n",
    "    g_index = Q.get_state_index(goal_col_index, goal_row_index)\n",
    "    action_trace.append((start_col, start_row))\n",
    "    while (s_index != g_index):\n",
    "        _,_,a_index = get_max_Q_action(Q, s_index)\n",
    "        action_trace.append(Q.get_grid_position(a_index))\n",
    "        s_index = a_index\n",
    "    return action_trace\n",
    "\n",
    "print retrieve_optimal_policy(Q_current, start_col_index, start_row_index, goal_col_index, goal_row_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
